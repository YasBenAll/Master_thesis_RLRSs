{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "dataset = \"SlateTopKBoredInfv0numitem100slatesize10_oracle_epsilon0.5_seed2023_n_users10\"\n",
    "data = torch.load(os.path.join(\"data\", \"datasets\", f\"{dataset}.pt\"))\n",
    "item_embeddings = torch.load(os.path.join(\"data\", \"datasets\", \"embeddings\", f\"{dataset}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataset where each row is a user containing a dict with the keys \"slate\" and \"clicks\"\n",
    "data = torch.load(os.path.join(\"data\", \"datasets\", f\"{dataset}.pt\")).observations\n",
    "\n",
    "# now there are no users defined, but they can be inferred as each 100 interactions is a user\n",
    "# so we can create a list of users where each user is a dict with the keys \"slate\" and \"clicks\"\n",
    "users = []\n",
    "user = dict(slate=[], clicks=[])\n",
    "for slate, clicks in zip(data[\"slate\"], data[\"clicks\"]):\n",
    "    user[\"slate\"].append(slate)\n",
    "    user[\"clicks\"].append(clicks)\n",
    "    if len(user[\"slate\"]) == 100:\n",
    "        users.append(user)\n",
    "        user = dict(slate=[], clicks=[])\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0., 25.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          5.,  0.,  0.,  0.,  7.,  0.,  0., 16.,  0., 14.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  2.,  0.,  0.,  0.,  6.,  0., 15.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 19.,  0., 17.,  0.,  0.,  0.,\n",
       "          5.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  6.,  0.,  0.,  0.,  0.,  0., 13.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  4.,  0., 22.,  0.,  0.,  0., 12.,  0.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0., 19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  9.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0., 16.,  1., 15.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0., 13.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0., 12.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0., 12.,  0., 24.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 13.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.,\n",
       "          9.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0., 27.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 14.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 13.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 25.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0., 24.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0., 21.,  0., 14.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0., 12.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         27.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 16.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 23.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  9.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0., 26.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0., 21.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 18.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  8., 12.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  9.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0., 11.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 12.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 11.,  0.,  0.,\n",
       "          0., 19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0., 11.,  0.,  0.,  2.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 11.,  0.,\n",
       "          0.,  0.],\n",
       "        [ 0.,  0.,  0.,  7.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 12.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 17.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,\n",
       "         18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 14.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.observations[\"slate\"]\n",
    "\n",
    "clicked_items_per_user = []\n",
    "\n",
    "user_list = []\n",
    "clicked_items = []\n",
    "\n",
    "for i in range(len(data.observations[\"slate\"])):\n",
    "    slate =data.observations[\"slate\"][i][0]\n",
    "    clicks = data.observations[\"clicks\"][i][0]\n",
    "    # if the item was clicked, add it to the list\n",
    "    for j in range(len(slate)):\n",
    "        if clicks[j] == 1:\n",
    "            clicked_items.append(int(slate[j]))\n",
    "    if (i + 1) % 100 == 0:\n",
    "        clicked_items_per_user.append(clicked_items)\n",
    "        clicked_items = [] \n",
    "\n",
    "\n",
    "# create a user matrix where the rows are the users and the columns are the items. The clicked items are marked as the number of times a user has clicked on them\n",
    "user_matrix = torch.zeros(len(clicked_items_per_user), 100)\n",
    "for i in range(len(clicked_items_per_user)):\n",
    "    for j in range(len(clicked_items_per_user[i])):\n",
    "        user_matrix[i][clicked_items_per_user[i][j]] += 1\n",
    "        \n",
    "user_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n",
      "(100, 10)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MatrixFactorization.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 190\u001b[0m\n\u001b[0;32m    187\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [1000]\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    193\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, ratings)\n",
      "File \u001b[1;32mc:\\Users\\Yassi\\Documents\\GitHub\\Master_thesis_RLRSs\\sardine\\embeddings\\semi_synthetic\\datasets-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Yassi\\Documents\\GitHub\\Master_thesis_RLRSs\\sardine\\embeddings\\semi_synthetic\\datasets-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: MatrixFactorization.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "\"\"\"Code based on https://github.com/naver/gems/blob/277a85fe971fdc736e4b292452b895631b4a1224/GeMS/modules/MatrixFactorization/\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming user_matrix and item_embeddings are already defined tensors\n",
    "# For example:\n",
    "# user_matrix = torch.randn(10, 100)        # 10 users, 100 items\n",
    "# item_embeddings = torch.randn(100, 10)    # 100 items, embedding dimension 10\n",
    "\n",
    "print(user_matrix.shape)       # Should be torch.Size([10, 100])\n",
    "print(item_embeddings.shape)   # Should be torch.Size([100, 10])\n",
    "\n",
    "def sample_items(num_items, shape):\n",
    "    \"\"\"\n",
    "    Randomly sample a number of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_items: int\n",
    "        Total number of items from which we should sample:\n",
    "        the maximum value of a sampled item id will be smaller\n",
    "        than this.\n",
    "    shape: int or tuple of ints\n",
    "        Shape of the sampled array.\n",
    "    Returns\n",
    "    -------\n",
    "    items: np.array of shape [shape]\n",
    "        Sampled item ids.\n",
    "    \"\"\"\n",
    "\n",
    "    res_items = np.random.randint(0, num_items, shape, dtype=np.int64)\n",
    "    return res_items\n",
    "\n",
    "class DotProdScorer(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(DotProdScorer, self).__init__()\n",
    "\n",
    "        # Define the device_ops\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, user_embeddings, item_embeddings):\n",
    "        # Scores based on the learned user/item embeddings\n",
    "        if self.training:\n",
    "            assert user_embeddings.size()[0] == item_embeddings.size()[0] # Equals to batch_size\n",
    "            # Score user-item pairs aligned in user_embeddings and item_embeddings\n",
    "            scores = (user_embeddings * item_embeddings).sum(-1).squeeze()\n",
    "            ## Shape of scores: (batch_size)\n",
    "        else:\n",
    "            # Score every pair made of a row from user_embeddings and a row from item_embeddings\n",
    "            scores = torch.mm(user_embeddings, item_embeddings.t())\n",
    "            ## Shape of scores: (batch_size, num_item)\n",
    "\n",
    "        return scores\n",
    "\n",
    "def bpr_loss(positive_score, negative_score):\n",
    "    \"\"\"\n",
    "    Bayesian Personalised Ranking loss\n",
    "    Args:\n",
    "        positive_score: (tensor<float>) predicted scores for known positive items\n",
    "        negative_score: (tensor<float>) predicted scores for negative sample items\n",
    "    Returns:\n",
    "        loss: (float) the mean value of the summed loss\n",
    "    \"\"\"\n",
    "    eps = 1e-7 # Smooth the argument of the log to prevent potential numerical underflows\n",
    "    loss = -torch.log(torch.sigmoid(positive_score - negative_score) + eps)\n",
    "    return loss.mean()\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    \"\"\"\n",
    "        Implementation of the matrix factorization with a BPR loss and trained with SGD\n",
    "    \"\"\"\n",
    "    def __init__(self, num_user, num_item, device_embed, device_ops, embedd_dim=10, lr_embedd=0.0001, num_neg_sample=1, weight_decay=0.0, train_val_split=0.1):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.embed_dim = embedd_dim\n",
    "        self.lr = lr_embedd\n",
    "        self.num_neg_sample = num_neg_sample\n",
    "        self.device_embed = device_embed\n",
    "        self.device_ops = device_ops\n",
    "        self.weight_decay = weight_decay\n",
    "        # Embeddings\n",
    "        self.user_embeddings = nn.Embedding(num_user, self.embed_dim) # User embeddings to be learned\n",
    "        self.item_embeddings = nn.Embedding(num_item, self.embed_dim) # Item embeddings to be learned\n",
    "        nn.init.xavier_uniform_(self.user_embeddings.weight, gain=1)\n",
    "        nn.init.xavier_uniform_(self.item_embeddings.weight, gain=1)\n",
    "        self.user_embeddings = self.user_embeddings.to(device_embed)\n",
    "        self.item_embeddings = self.item_embeddings.to(device_embed)\n",
    "\n",
    "        # Components of the model\n",
    "        self.scorer = DotProdScorer(device_ops).to(device_ops)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def predict(self, user_ids, item_ids=None):\n",
    "        \"\"\"\n",
    "        Compute the score predictions at test time\n",
    "        Args:\n",
    "            user_ids: (array<int>) users for whom to recommend items\n",
    "            item_ids: (array<int>) items for which prediction scores are desired; if not provided, predictions for all\n",
    "            items will be computed\n",
    "        Returns:\n",
    "            scores: (tensor<float>) predicted scores for all items in item_ids\n",
    "        \"\"\"\n",
    "        batch_user_embeddings = self.user_embeddings(user_ids).to(self.device_ops)\n",
    "        ## Shape of batch_user_embeddings: (batch_size, embed_dim)\n",
    "\n",
    "        if item_ids is None:\n",
    "            item_ids = np.arange(self.num_item)\n",
    "        item_ids = torch.tensor(item_ids, dtype=torch.long, device=self.device_embed)\n",
    "        batch_item_embeddings = self.item_embeddings(item_ids).to(self.device_ops)\n",
    "        ## Shape of batch_item_embeddings: (num_item, embed_dim)\n",
    "\n",
    "        scores = self.scorer(batch_user_embeddings, batch_item_embeddings)\n",
    "        ## Shape of scores: (batch_size, num_item)\n",
    "        return scores\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Unpack the content of the minibatch\n",
    "        user_ids = batch['user_ids']\n",
    "        item_ids = batch['item_ids']\n",
    "\n",
    "        # Fetch the user embeddings for the minibatch\n",
    "        batch_user_embeddings = self.user_embeddings(user_ids).to(self.device_ops)\n",
    "        ## Shape of batch_user_embeddings: (batch_size, embed_dim)\n",
    "\n",
    "        # Fetch the (positive) item embeddings for the minibatch\n",
    "        batch_item_embeddings = self.item_embeddings(item_ids).to(self.device_ops)\n",
    "        ## Shape of batch_item_embeddings: (batch_size, embed_dim)\n",
    "\n",
    "        # Calculate the recommendation loss on the minibatch using BPR\n",
    "        positive_score = self.scorer(batch_user_embeddings, batch_item_embeddings)\n",
    "\n",
    "        ## Shape of positive_score: (batch_size)\n",
    "        loss = torch.tensor(0.0, dtype=torch.float, device=self.device_ops)\n",
    "        for i in range(self.num_neg_sample):\n",
    "            # Negative sampling\n",
    "            negative_item_ids = sample_items(self.num_item, item_ids.size())\n",
    "            negative_item_ids = torch.tensor(negative_item_ids, dtype=torch.long, device=self.device_embed)\n",
    "            batch_negative_item_embeddings = self.item_embeddings(negative_item_ids).to(self.device_ops)\n",
    "            ## Shape of batch_negative_item_embeddings: (batch_size, embed_dim)\n",
    "            negative_score = self.scorer(batch_user_embeddings, batch_negative_item_embeddings)\n",
    "            ## Shape of negative_score: (batch_size)\n",
    "            # Compute the BPR loss on the positive and negative scores while masking padded elements in the sequences\n",
    "            loss += bpr_loss(positive_score, negative_score)\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Correctly assign number of items\n",
    "n_users = user_matrix.size(0)          # 10\n",
    "n_items = item_embeddings.shape[0]     # Corrected: use shape to get the first dimension\n",
    "embedding_dim = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MatrixFactorization(n_users, n_items, device, device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare training data: all possible user-item pairs\n",
    "# Number of pairs: n_users * n_items\n",
    "user_indices = torch.LongTensor([u for u in range(n_users) for _ in range(n_items)])  # Shape: [1000]\n",
    "item_indices = torch.LongTensor([i for i in range(n_items) for _ in range(n_users)])  # Shape: [1000]\n",
    "ratings = user_matrix.view(-1)   \n",
    "                                            # Shape: [1000]\n",
    "\n",
    "# Optionally, move data to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "user_indices = user_indices.to(device)\n",
    "item_indices = item_indices.to(device)\n",
    "ratings = ratings.to(device)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(n_users, n_items)  # Shape: [1000]\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, ratings)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Retrieve the trained user embeddings\n",
    "user_embeddings = model.user_embedding.weight.data\n",
    "print(user_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3152781 , 0.        , 0.        , 0.94899932],\n",
       "       [0.        , 0.40346547, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.649015  , 0.64497685],\n",
       "       [0.        , 0.41663766, 0.69163858, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.58995689, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33808886, 0.        , 0.        , 0.94111419],\n",
       "       [0.95641525, 0.        , 0.        , 0.        , 0.29201005,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.03330959, 0.        , 0.8047671 , 0.59265537],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.86394508, 0.50358605, 0.        , 0.        , 0.        ],\n",
       "       [0.1750753 , 0.84795926, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.50031363, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.90706494,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.42099073],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.69875723,\n",
       "        0.        , 0.5883863 , 0.        , 0.        , 0.40686594],\n",
       "       [0.78925887, 0.        , 0.        , 0.        , 0.59356479,\n",
       "        0.        , 0.        , 0.        , 0.15732537, 0.        ],\n",
       "       [0.        , 0.81486681, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.57964823, 0.        ],\n",
       "       [0.        , 0.0363653 , 0.        , 0.12980274, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.99087275],\n",
       "       [0.        , 0.1186542 , 0.        , 0.        , 0.        ,\n",
       "        0.7422082 , 0.        , 0.        , 0.        , 0.65958181],\n",
       "       [0.75466514, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.10807783, 0.64714736, 0.        , 0.        ],\n",
       "       [0.        , 0.11057449, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.99386784],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.7260511 ,\n",
       "        0.        , 0.68764075, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.76864478,\n",
       "        0.        , 0.        , 0.        , 0.63967586, 0.        ],\n",
       "       [0.        , 0.70470932, 0.        , 0.        , 0.        ,\n",
       "        0.70949614, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.33025548, 0.94389158,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.27758061, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.96070235],\n",
       "       [0.        , 0.        , 0.        , 0.99697386, 0.        ,\n",
       "        0.        , 0.        , 0.07773758, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.86677169, 0.4924284 ,\n",
       "        0.        , 0.        , 0.07887406, 0.        , 0.        ],\n",
       "       [0.39628664, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.91812684, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.96728804, 0.        , 0.        ,\n",
       "        0.        , 0.25368061, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.64489762, 0.        , 0.        , 0.76426897,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.66205166, 0.        , 0.        ,\n",
       "        0.        , 0.74945821, 0.        , 0.        , 0.        ],\n",
       "       [0.47069939, 0.        , 0.        , 0.42614844, 0.        ,\n",
       "        0.77255394, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.6812034 , 0.        , 0.        , 0.3932279 , 0.        ,\n",
       "        0.        , 0.61752227, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.64706877, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.76243164, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.61291687, 0.        ,\n",
       "        0.        , 0.        , 0.59337402, 0.52176641, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.91915089, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.39390562],\n",
       "       [0.        , 0.        , 0.57433748, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.81861863],\n",
       "       [0.        , 0.86614044, 0.        , 0.        , 0.4998007 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.63351258, 0.62915802, 0.        , 0.45035762, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.97415869, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.22586467, 0.        ],\n",
       "       [0.        , 0.        , 0.08886046, 0.        , 0.        ,\n",
       "        0.        , 0.68671329, 0.        , 0.72147673, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.99207967, 0.        ,\n",
       "        0.        , 0.        , 0.12561019, 0.        , 0.        ],\n",
       "       [0.76874661, 0.        , 0.        , 0.63955347, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.93816908, 0.        , 0.        , 0.        , 0.34617738],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.94856156,\n",
       "        0.        , 0.        , 0.31659275, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.0214395 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.99977015, 0.        , 0.        ],\n",
       "       [0.20404149, 0.        , 0.        , 0.        , 0.71940107,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.66394968],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32359263, 0.        , 0.        , 0.9461965 ],\n",
       "       [0.        , 0.15022361, 0.24282006, 0.95836908, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.08814423,\n",
       "        0.        , 0.80636644, 0.        , 0.        , 0.58481088],\n",
       "       [0.37838425, 0.71762557, 0.        , 0.        , 0.        ,\n",
       "        0.58466991, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07011159, 0.        , 0.99753915, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.36656413, 0.        , 0.        , 0.        , 0.93039279],\n",
       "       [0.74218712, 0.        , 0.        , 0.57597664, 0.        ,\n",
       "        0.        , 0.34265023, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.78018767, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.62554552],\n",
       "       [0.        , 0.        , 0.        , 0.64676672, 0.4802769 ,\n",
       "        0.        , 0.        , 0.        , 0.59247524, 0.        ],\n",
       "       [0.32685305, 0.        , 0.        , 0.        , 0.94507517,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.64990163,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.76001834],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.94290461, 0.        , 0.        , 0.33306289, 0.        ],\n",
       "       [0.        , 0.41411449, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.9102248 ],\n",
       "       [0.        , 0.64681693, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.08848336, 0.75749492, 0.        ],\n",
       "       [0.        , 0.96087386, 0.        , 0.        , 0.        ,\n",
       "        0.22923745, 0.15547224, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.16953882, 0.        , 0.75692991, 0.        , 0.63112098],\n",
       "       [0.7150381 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5609261 , 0.        , 0.41723186, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28276322, 0.        , 0.95918974, 0.        ],\n",
       "       [0.        , 0.        , 0.7036593 , 0.        , 0.58760834,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.39947469],\n",
       "       [0.        , 0.        , 0.        , 0.10068499, 0.70276864,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.70425761],\n",
       "       [0.68637451, 0.68653608, 0.        , 0.        , 0.        ,\n",
       "        0.239913  , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.6031804 , 0.61071236, 0.        , 0.        ,\n",
       "        0.        , 0.51303394, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.66085651,\n",
       "        0.34774306, 0.        , 0.66508904, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.99244563, 0.        ,\n",
       "        0.12268527, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.02389663, 0.        , 0.99971443, 0.        ],\n",
       "       [0.        , 0.        , 0.73498867, 0.67807938, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.86231091, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5063792 , 0.        , 0.        , 0.        ],\n",
       "       [0.12581018, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.96054409, 0.        , 0.        , 0.24804605, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.31740139, 0.        , 0.        , 0.94829128, 0.        ],\n",
       "       [0.        , 0.94927413, 0.2845014 , 0.        , 0.        ,\n",
       "        0.        , 0.13393126, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08372953, 0.        , 0.99648852, 0.        , 0.        ],\n",
       "       [0.28791935, 0.        , 0.95765466, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.23047707, 0.        , 0.        , 0.        , 0.97307776,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.82009187, 0.        , 0.        , 0.57223188, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.82367612,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.56706053],\n",
       "       [0.5774627 , 0.09863787, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.81043655, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.71050912, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.70368799],\n",
       "       [0.        , 0.36657101, 0.        , 0.48931875, 0.        ,\n",
       "        0.        , 0.79132349, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.37919166,\n",
       "        0.        , 0.        , 0.        , 0.92531815, 0.        ],\n",
       "       [0.        , 0.        , 0.90466105, 0.        , 0.        ,\n",
       "        0.        , 0.31204306, 0.29020254, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.65466861, 0.75591601],\n",
       "       [0.        , 0.8828282 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.46969604],\n",
       "       [0.        , 0.44878033, 0.        , 0.        , 0.32727094,\n",
       "        0.83155875, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.2930643 , 0.95609274, 0.        , 0.        ],\n",
       "       [0.        , 0.95314132, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.17892997, 0.2439379 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.92323274,\n",
       "        0.34469257, 0.1697891 , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.72103461, 0.        , 0.        ,\n",
       "        0.        , 0.69289905, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.54064633, 0.        , 0.65007627,\n",
       "        0.5339498 , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.97685417,\n",
       "        0.        , 0.21390637, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.13319318, 0.99109009, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.56362897, 0.        ,\n",
       "        0.        , 0.        , 0.77951369, 0.        , 0.27327785],\n",
       "       [0.        , 0.        , 0.        , 0.22391343, 0.        ,\n",
       "        0.        , 0.        , 0.97460904, 0.        , 0.        ],\n",
       "       [0.17123038, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.35814446, 0.        , 0.91783043],\n",
       "       [0.86424907, 0.        , 0.        , 0.        , 0.50306415,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.4291178 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.90324854],\n",
       "       [0.        , 0.        , 0.28192576, 0.        , 0.95943622,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.06572181, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.39104036, 0.        , 0.91802401]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4938, 0.2779, 0.3481, 0.6168, 0.3508], grad_fn=<SelectBackward0>)\n",
      "tensor([0.6394, 0.4799, 0.2776, 0.6288, 0.4236], grad_fn=<SelectBackward0>)\n",
      "tensor([0.5125, 0.2788, 0.4804, 0.3227, 0.2684], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.item_embedding.weight[0])\n",
    "print(model.item_embedding.weight[3])\n",
    "print(model.item_embedding.weight[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 9., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 0, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.84526873, 0.09005205, 0.        , 0.52669858],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.84526873, 0.09005205, 0.        , 0.52669858],\n",
       "       [0.41056857, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.91182973, 0.        ],\n",
       "       [0.        , 0.        , 0.70198059, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1866325 , 0.68730747, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5942942 , 0.55739437, 0.57976367, 0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get random elements from item_embeddings\n",
    "samples = item_embeddings[np.random.randint(0, item_embeddings.shape[0], 5)]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_items(num_items, shape):\n",
    "    \"\"\"\n",
    "    Randomly sample a number of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_items: int\n",
    "        Total number of items from which we should sample:\n",
    "        the maximum value of a sampled item id will be smaller\n",
    "        than this.\n",
    "    shape: int or tuple of ints\n",
    "        Shape of the sampled array.\n",
    "    Returns\n",
    "    -------\n",
    "    items: np.array of shape [shape]\n",
    "        Sampled item ids.\n",
    "    \"\"\"\n",
    "\n",
    "    res_items = np.random.randint(0, num_items, shape, dtype=np.int64)\n",
    "    return res_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "torch.tensor([b[0] for b in user_matrix], dtype = torch.long,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 5, 14, 18, 21, 23, 39, 56, 59, 63, 65, 66, 83]),\n",
       " tensor([10, 22, 24, 28, 34, 58, 64, 89, 91, 95]),\n",
       " tensor([ 4, 24, 35, 45, 58, 59, 60, 72, 84]),\n",
       " tensor([ 5, 11, 21, 23, 39, 65, 70, 82]),\n",
       " tensor([30, 40, 53, 82, 96]),\n",
       " tensor([ 4, 35, 45, 58, 60, 62, 72, 78]),\n",
       " tensor([ 6, 17, 22, 42, 46, 53, 64, 80, 92]),\n",
       " tensor([ 1, 13, 21, 23, 30, 39, 52, 53, 57, 65, 66, 77, 96]),\n",
       " tensor([ 1, 13, 30, 41, 53, 57, 77, 80, 96]),\n",
       " tensor([ 3, 11, 23, 30, 40, 50, 65, 70, 82])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = user_matrix\n",
    "# assign each user in the data a unique id\n",
    "data = [(i, data[i]) for i in range(len(data))]\n",
    "[torch.nonzero(b[1], as_tuple=True)[0] for b in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  25.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  5.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  7.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  16.0,\n",
       "  0.0,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  10.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  15.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  7.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  19.0,\n",
       "  0.0,\n",
       "  17.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  5.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  13.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  4.0,\n",
       "  0.0,\n",
       "  22.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  12.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  19.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  9.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  12.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  16.0,\n",
       "  1.0,\n",
       "  15.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  13.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  12.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  5.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  12.0,\n",
       "  0.0,\n",
       "  24.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  13.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  20.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  9.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  4.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  27.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  10.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  13.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  25.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  24.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  7.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  21.0,\n",
       "  0.0,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  12.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  27.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  20.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  16.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  23.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  9.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  26.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  21.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  18.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.0,\n",
       "  12.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  9.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  11.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  7.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  12.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  11.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  19.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  11.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  11.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  7.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  12.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  17.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  10.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  18.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b[1].tolist() for b in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('datasets-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e33288291614559f708eefa23cec1f90df7675d535b8e8f151f74a84b29fc24e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
