{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity_threshold 2\n",
      "num_items: 20, num_users: 1000, num_actions: 20, slate_size: 10\n",
      "2\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "test\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'divide' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 565\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m## Generate a dataset of 10 users with 50% random actions and 50% greedy actions\u001b[39;00m\n\u001b[1;32m    564\u001b[0m logging_policy \u001b[38;5;241m=\u001b[39m EpsilonGreedyOracle(epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, env \u001b[38;5;241m=\u001b[39m env, seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2023\u001b[39m)\n\u001b[0;32m--> 565\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_users\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogging_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m env\u001b[38;5;241m.\u001b[39mget_item_embeddings()\n",
      "Cell \u001b[0;32mIn[1], line 436\u001b[0m, in \u001b[0;36mSardine.generate_dataset\u001b[0;34m(self, n_users, policy, seed, dataset_type)\u001b[0m\n\u001b[1;32m    434\u001b[0m dataset \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 436\u001b[0m observation, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m    438\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 208\u001b[0m, in \u001b[0;36mSardine.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    205\u001b[0m     relevances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(norm_score \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_threshold, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m## First interaction\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m clicks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_clicks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelevances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_comp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m clicked_items \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(clicks)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclicked_items\u001b[38;5;241m.\u001b[39mextend(slate[clicked_items])\n",
      "Cell \u001b[0;32mIn[1], line 149\u001b[0m, in \u001b[0;36mSardine._compute_clicks\u001b[0;34m(self, rels, comps)\u001b[0m\n\u001b[1;32m    147\u001b[0m attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m rels\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39munique(comps, return_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiversity_threshold:\n\u001b[0;32m--> 149\u001b[0m     \u001b[43mattr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiversity_penalty\u001b[49m \u001b[38;5;66;03m### When too many similar are in the slate, the overall attractiveness of the slate decreases.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m click_probs \u001b[38;5;241m=\u001b[39m attr \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropensities\n\u001b[1;32m    151\u001b[0m clicks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random\u001b[38;5;241m.\u001b[39mbinomial(n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, p \u001b[38;5;241m=\u001b[39m click_probs)\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'divide' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Literal\n",
    "from policies import Policy\n",
    "from collections import deque, OrderedDict\n",
    "\n",
    "from consts import DATA_REC_SIM_EMBEDDS\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Pre-defined boredom types and click models\n",
    "_BOREDOM_TYPES = Literal[\"user_tloi\", \"user_car\"]\n",
    "_CLICK_MODELS = Literal[\"tdPBM\", \"mixPBM\"]\n",
    "_DATASET_FORMATS = Literal[\"dict\", \"sb3_rollout\", \"sb3_replay\"]\n",
    "\n",
    "class Sardine(gym.Env):\n",
    "    '''\n",
    "        a Simulator for Automated Recommendation in Dynamic and INteractive Environments\n",
    "    '''\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, num_items : int, slate_size : int, num_topics : int, episode_length : int, \n",
    "                env_alpha : float, env_propensities : List[float], env_offset : float, env_slope: float, env_omega : float, \n",
    "                recent_items_maxlen : int, boredom_threshold : int, boredom_moving_window : int, \n",
    "                env_embedds : str, click_model : _CLICK_MODELS, rel_threshold : float, \n",
    "                diversity_penalty : float, diversity_threshold : int, click_prop : float,\n",
    "                boredom_type : _BOREDOM_TYPES, rel_penalty : bool, render_mode=None, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        ### General parameters of the environment\n",
    "        self.num_items = num_items\n",
    "        self.item_ids = np.arange(self.num_items)  # Simple sequential IDs\n",
    "        self.item_occurrences = np.zeros(self.num_items, dtype=int)\n",
    "        self.item_clicks = np.zeros(self.num_items, dtype=int)\n",
    "        self.num_topics = num_topics\n",
    "        self.slate_size = slate_size\n",
    "        self.H = episode_length\n",
    "\n",
    "        ### Observation and action spaces\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"slate\": spaces.MultiDiscrete([num_items] * slate_size),\n",
    "                \"clicks\": spaces.MultiBinary(n = slate_size),\n",
    "                \"hist\": spaces.Box(low = 0, high = 1, shape=(num_topics,), dtype=np.float32)\n",
    "            }\n",
    "        )\n",
    "        self.action_space = spaces.MultiDiscrete([num_items] * slate_size)\n",
    "\n",
    "        ### Click model\n",
    "        self._set_propensities(click_model, click_prop, env_propensities, env_alpha)\n",
    "\n",
    "        ### User preference model\n",
    "        self.offset = env_offset\n",
    "        self.slope = env_slope\n",
    "        self.omega = env_omega\n",
    "        self.rel_threshold = float(rel_threshold)\n",
    "        self.diversity_penalty = float(diversity_penalty)\n",
    "        self.diversity_threshold = diversity_threshold\n",
    "        print(self.diversity_threshold)\n",
    "\n",
    "        ### Boredom model\n",
    "        self.recent_items_maxlen = recent_items_maxlen\n",
    "        self.boredom_thresh = boredom_threshold\n",
    "        self.boredom_moving_window = boredom_moving_window\n",
    "\n",
    "        self.rel_penalty = rel_penalty\n",
    "        self.boredom_type = boredom_type\n",
    "\n",
    "        ### Item generation\n",
    "        self._init_item_embeddings(env_embedds)\n",
    "        self._set_topic_for_items()\n",
    "\n",
    "    def get_item_embeddings(self):\n",
    "        \"\"\"\n",
    "        Returns a copy of the item embeddings array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The item embeddings.\n",
    "        \"\"\"\n",
    "        return np.copy(self.item_embedd)\n",
    "\n",
    "    def get_item_occurrences(self):\n",
    "        return self.item_occurrences\n",
    "\n",
    "    def get_item_clicks(self):\n",
    "        return self.item_clicks\n",
    "\n",
    "    def _set_propensities(self, click_model: str, click_prop : float = None, env_propensities : List[float] = None, env_alpha : float = 1.0):\n",
    "        '''\n",
    "            Setting click propensities of the PBM click model.\n",
    "        '''\n",
    "        self.alpha = float(env_alpha) # Discount applied from relevance to attractiveness\n",
    "\n",
    "        if click_model==\"tdPBM\":\n",
    "            self.propensities = np.power(click_prop, np.arange(self.slate_size))\n",
    "        elif click_model==\"mixPBM\":\n",
    "            probs = [0.5, 0.5]\n",
    "            props = np.power(click_prop, np.arange(self.slate_size))\n",
    "            #props = gammas[:, np.newaxis].repeat(self.slate_size, axis = 1)\n",
    "            self.propensities = probs[0] * props + probs[1] * np.flip(props, axis = 0)\n",
    "        else:\n",
    "            self.propensities = env_propensities\n",
    "\n",
    "    def _init_item_embeddings(self, env_embedds : str):\n",
    "        \"\"\"\n",
    "        Initializes item embeddings\n",
    "        \"\"\"\n",
    "        if env_embedds is None: # Generate new item embeddings\n",
    "            # Item embeddings with only a certain number of topics\n",
    "            # Values for other topics are completely zeroed out\n",
    "            num_topics_per_item = 2 # Average number of topics per item\n",
    "            self.item_embedd = np.random.rand(self.num_items, self.num_topics)\n",
    "\n",
    "            # Create a boolean tensor of shape [num_items, num_topics] filled with False values\n",
    "            mask = np.zeros((self.num_items, self.num_topics), dtype=bool)\n",
    "            # Force items to have between 2 and 3 topics\n",
    "            # Since all items which have single topics are identical to each other (i.e., same one-hot vector),\n",
    "            # Utilize items between 2 and 3 topics\n",
    "            num_true_values = np.random.randint(num_topics_per_item, num_topics_per_item + 2, (self.num_items,))\n",
    "            for i in range(self.num_items):\n",
    "                indices = np.random.permutation(self.num_topics)[:num_true_values[i]]\n",
    "                mask[i, indices] = True\n",
    "            self.item_embedd *= mask\n",
    "            embedd_norm = np.linalg.norm(self.item_embedd, axis = -1)\n",
    "\n",
    "            self.item_embedd /= embedd_norm[:, np.newaxis]\n",
    "            np.save(os.path.join(\"embeddings\", \"item_embeddings\"), self.item_embedd)\n",
    "            np.savetxt(\"DEV_item_embeddings.csv\", self.item_embedd, delimiter=\",\")\n",
    "        else: # Load existing item embeddings\n",
    "            self.item_embedd = np.load(os.path.join(DATA_REC_SIM_EMBEDDS, env_embedds))\n",
    "\n",
    "    def _set_topic_for_items(self):\n",
    "        \"\"\"\n",
    "        Sets main topic for each item\n",
    "        \"\"\"\n",
    "        # with m > 1:\n",
    "        self.item_comp = np.argmax(self.item_embedd, axis = 1)\n",
    "        self.max_score = np.max(np.linalg.norm(self.item_embedd, axis = 1))\n",
    "\n",
    "    def _compute_clicks(self, rels : np.ndarray, comps : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "            PBM click model\n",
    "        '''\n",
    "        attr = self.alpha * rels\n",
    "        if np.max(np.unique(comps, return_counts = True)[1]) >= self.diversity_threshold:\n",
    "            attr /= self.diversity_penalty ### When too many similar are in the slate, the overall attractiveness of the slate decreases.\n",
    "        click_probs = attr * self.propensities\n",
    "        clicks = self.np_random.binomial(n = 1, p = click_probs)\n",
    "        return clicks\n",
    "\n",
    "    def _reset_user_embedds(self):\n",
    "        '''\n",
    "            Resets the user embedding\n",
    "        '''\n",
    "        # User embedding where users are only interested in a certain number of topics\n",
    "        # Values for other topics are completely zeroed out\n",
    "\n",
    "        num_topics_per_user = 4 # Average number of topics per user\n",
    "        threshold = 1 - float(num_topics_per_user) / self.num_topics\n",
    "        self.user_embedd = self.np_random.uniform(size = (self.num_topics,))\n",
    "        mask = self.np_random.uniform(size = (self.num_topics,)) > threshold\n",
    "        while sum(mask) <= num_topics_per_user - 2 or sum(mask) >= num_topics_per_user + 2: # Force users to have between 3 and 5 topics\n",
    "            mask = self.np_random.uniform(size = (self.num_topics,)) > threshold\n",
    "        self.user_embedd *= mask\n",
    "        embedd_norm = np.linalg.norm(self.user_embedd)\n",
    "        self.user_embedd /= embedd_norm\n",
    "\n",
    "    def _initial_reco(self):\n",
    "        \"\"\"\n",
    "        Initial slate recommendation with random items\n",
    "        \"\"\"\n",
    "        return self.np_random.integers(low = 0, high = self.item_embedd.shape[0], size = (self.slate_size,))\n",
    "\n",
    "    def reset(self, seed = None, options = None) -> Tuple[Dict, Dict]:\n",
    "        '''\n",
    "            The initial ranker returns the most qualitative document in each topic (or the 10 first topics, or multiple top_docs per topic)\n",
    "        '''\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.boredom_counter = 0\n",
    "        self.t = 0  # Index of the trajectory-wide timestep\n",
    "        self.clicked_items = deque([], self.recent_items_maxlen)\n",
    "        self.clicked_item_topics = deque([], self.recent_items_maxlen)\n",
    "        self.clicked_step = deque([], self.recent_items_maxlen)\n",
    "        self.all_clicked_items = []\n",
    "        self.bored = np.zeros(self.num_topics, dtype = bool)\n",
    "        self.bored_timeout = self.boredom_moving_window * np.ones(self.num_topics, dtype = int)\n",
    "\n",
    "        ## User embeddings\n",
    "        self._reset_user_embedds()\n",
    "\n",
    "        ## Initial recommendation\n",
    "        slate = self._initial_reco()\n",
    "\n",
    "        ## Compute relevances\n",
    "        slate_embedd = self.item_embedd[slate]    # slate_size, num_topics\n",
    "        score = slate_embedd @ self.user_embedd   # slate_size\n",
    "        norm_score = score / self.max_score # Normalize score\n",
    "        if self.rel_threshold is None:\n",
    "            relevances = 1 / (1 + np.exp(-(norm_score - self.offset) * self.slope))    ## Rescale relevance\n",
    "        else:\n",
    "            relevances = np.where(norm_score > self.rel_threshold, 1, 0)\n",
    "\n",
    "        ## First interaction\n",
    "        clicks = self._compute_clicks(relevances, self.item_comp[slate])\n",
    "        clicked_items = np.where(clicks)[0]\n",
    "        self.clicked_items.extend(slate[clicked_items])\n",
    "        self.clicked_item_topics.extend(self.item_comp[slate[clicked_items]])\n",
    "        self.clicked_step.extend(self.t * np.ones_like(clicked_items))\n",
    "        self.all_clicked_items.extend(slate[clicked_items])\n",
    "\n",
    "        ## Update the user state for the next step\n",
    "        user_state = self._update_user_state(slate, clicked_items)\n",
    "\n",
    "        info = {'user_state' : user_state, 'terminated' : False, 'clicks' : clicks}\n",
    "        obs = {'slate' : slate, 'clicks' : clicks, 'hist' : self.norm_recent_topics_hist}\n",
    "        return obs, info\n",
    "\n",
    "    def get_st_oracle_slate(self, slate, relevances):\n",
    "        \"\"\"\n",
    "        Short term oracle for a given user embedding.\n",
    "        Completes a slate whose missing elements are replaced with -1\n",
    "        \"\"\"\n",
    "        ind = np.argpartition(relevances, - self.slate_size)[- self.slate_size:]\n",
    "        topk_relevances = np.argsort(relevances[ind])\n",
    "        oracle_slate = ind[np.flip(topk_relevances)]\n",
    "        antioracle_slate = ind[topk_relevances]\n",
    "\n",
    "        return np.where(slate == -1, oracle_slate, np.where(slate == -2, antioracle_slate, slate))\n",
    "\n",
    "    def _adjust_user_embedds(self, cur_u_embedd, bored_topics):\n",
    "        # Boredom factor influences the user embeddings\n",
    "        if self.boredom_type == \"user_tloi\": # Temporary loss-of-interest boredom: boring topic components are zeroed out\n",
    "            ### Set bored component to 0\n",
    "            for bt in bored_topics:\n",
    "                cur_u_embedd[bt] = 0.0\n",
    "\n",
    "        if self.boredom_type == \"user_car\": # Churn-and-return boredom: all components are zeroed out\n",
    "            ### Set user embedding to 0 if there is any boredom\n",
    "            if (self.bored == True).sum() > 0:\n",
    "                cur_u_embedd[:] = 0.0\n",
    "\n",
    "        return cur_u_embedd\n",
    "\n",
    "    def _clicked_item_influence(self, slate, clicked_items):\n",
    "        \"\"\"\n",
    "        Influence the user embedding with the clicked items in the slate\n",
    "        \"\"\"\n",
    "        if len(slate[clicked_items]) > 0:\n",
    "            # Compute the average of item embeddings for the clicked items in the slate\n",
    "            slate_item_embedd = np.mean([self.item_embedd[it] for it in slate[clicked_items]])\n",
    "\n",
    "            self.user_embedd = self.omega * self.user_embedd + (1 - self.omega) * slate_item_embedd\n",
    "            embedd_norm = np.linalg.norm(self.user_embedd)\n",
    "            self.user_embedd /= embedd_norm\n",
    "\n",
    "    def _update_user_state(self, slate, clicked_items):\n",
    "        ## Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        ## We remove old clicks from boredom \"log\"\n",
    "        while len(self.clicked_step) > 0 and self.clicked_step[0] < self.t - self.boredom_moving_window:\n",
    "            self.clicked_item_topics.popleft()\n",
    "            self.clicked_step.popleft()\n",
    "        self.all_clicked_items.extend(slate[clicked_items])\n",
    "\n",
    "        ## Update bored_timeout in the next step\n",
    "        self.bored_timeout -= self.bored.astype(int) # Remove one to timeout for bored topics\n",
    "        self.bored = self.bored & (self.bored_timeout != 0) # \"Unbore\" timed out components\n",
    "        self.bored_timeout[self.bored == False] = self.boredom_moving_window # Reset timer for \"unbored\" components\n",
    "\n",
    "        ## Bored anytime recently items from one topic have been clicked more than boredom_threshold\n",
    "        if len(self.clicked_item_topics) > 0:\n",
    "            recent_topics = np.concatenate([it[np.newaxis] for it in self.clicked_item_topics])\n",
    "            recent_topics_hist = np.bincount(recent_topics, minlength = self.num_topics)\n",
    "            bored_topics = np.arange(self.num_topics)[recent_topics_hist >= self.boredom_thresh]\n",
    "            ## Then, boredom is triggered for the topics on which have been clicked more than boredom_thresh\n",
    "            self.bored[bored_topics] = True\n",
    "        else:\n",
    "            ## No clicked items in recent history\n",
    "            recent_topics_hist = np.zeros(self.num_topics)\n",
    "        bored_topics = np.nonzero(self.bored)[0]\n",
    "\n",
    "        ## Let clicked items influence user behavior\n",
    "        self._clicked_item_influence(slate, clicked_items)\n",
    "\n",
    "        ## Apply boredom and short-term interest to the user embedding\n",
    "        self.cur_user_embedd = self._adjust_user_embedds(self.user_embedd.copy(), bored_topics)\n",
    "\n",
    "        ## Define user state and normalize vectors between 0 and 1\n",
    "        self.norm_recent_topics_hist = np.clip(recent_topics_hist / self.boredom_thresh, 0, 1).astype('float32') # Clip between 0 and 1\n",
    "        norm_bored_timeout = self.bored_timeout / self.boredom_moving_window\n",
    "        bored = self.bored.astype(np.float32)\n",
    "        user_state = np.concatenate([self.cur_user_embedd, self.norm_recent_topics_hist, norm_bored_timeout], axis=0, dtype=np.float32)\n",
    "        #user_state = np.concatenate([self.user_embedd, self.norm_recent_topics_hist, norm_bored_timeout], axis=0, dtype=np.float32)\n",
    "        #user_state = np.concatenate([self.cur_user_embedd, self.norm_recent_topics_hist, norm_bored_timeout, bored], axis=0, dtype=np.float32)\n",
    "\n",
    "        return user_state\n",
    "\n",
    "    def step(self, slate, condition=False) -> Tuple[Dict, float, bool, bool, Dict]:\n",
    "        '''\n",
    "            Simulates user interaction.\n",
    "        '''\n",
    "        \n",
    "\n",
    "        ## Compute relevances\n",
    "        scores = self.item_embedd @ self.cur_user_embedd\n",
    "\n",
    "        norm_scores = scores / self.max_score # Normalize score\n",
    "        if self.rel_threshold is None:\n",
    "            relevances = 1 / (1 + np.exp(-(norm_scores - self.offset) * self.slope))    ## Rescale relevance\n",
    "        else:\n",
    "            relevances = np.where(norm_scores > self.rel_threshold, 1, 0)\n",
    "\n",
    "        ## Reduce overall relevance score if user is bored => Reflects interest in platform\n",
    "        if self.rel_penalty:\n",
    "            relevances *= (0.5 ** (self.bored == True).sum())\n",
    "\n",
    "        # Short Time Oracle acts as greedily as possible.\n",
    "        st_oracle = (-1 in slate) or (-2 in slate)\n",
    "        if st_oracle:\n",
    "            slate = self.get_st_oracle_slate(slate, relevances)\n",
    "        relevances = relevances[slate]\n",
    "\n",
    "        info = {}\n",
    "        info[\"slate\"] = slate\n",
    "        info[\"slate_components\"] = self.item_comp[slate]\n",
    "        info[\"scores\"] = norm_scores[slate]\n",
    "        info[\"bored\"] = self.bored\n",
    "        info[\"relevances\"] = relevances\n",
    "\n",
    "        ## Interaction\n",
    "        clicks = self._compute_clicks(relevances, self.item_comp[slate])\n",
    "        clicked_items = np.where(clicks)[0]\n",
    "        self.clicked_items.extend(slate[clicked_items])\n",
    "        self.clicked_item_topics.extend(self.item_comp[slate[clicked_items]])\n",
    "        self.clicked_step.extend(self.t * np.ones_like(clicked_items))\n",
    "        info[\"clicks\"] = clicks\n",
    "\n",
    "\n",
    "        # Update occurrences\n",
    "        for item_id in slate:\n",
    "            self.item_occurrences[item_id] += 1\n",
    "\n",
    "        # Update clicks\n",
    "        for idx, click in enumerate(clicks):\n",
    "            if click == 1:\n",
    "                self.item_clicks[action[idx]] += 1\n",
    "\n",
    "\n",
    "\n",
    "        print(action)\n",
    "\n",
    "        ## Update the user state for the next step\n",
    "        user_state = self._update_user_state(slate, clicked_items)\n",
    "        info[\"user_state\"] = user_state\n",
    "\n",
    "        ## Set terminated and return\n",
    "        if self.t > self.H:\n",
    "            terminated = True\n",
    "            info[\"terminated\"] = True\n",
    "        else:\n",
    "            terminated = False\n",
    "            info[\"terminated\"] = False\n",
    "\n",
    "        obs = {'slate' : slate, 'clicks' : clicks, 'hist' : self.norm_recent_topics_hist}\n",
    "\n",
    "        if not condition:\n",
    "            reward = np.sum(clicks)\n",
    "        else:\n",
    "            \"\"\"\n",
    "                TODO : Implement a reward function for the conditional case\n",
    "            \"\"\"\n",
    "\n",
    "            r_acc = np.sum(clicks)\n",
    "\n",
    "            def intra_slate_dissimilarity(slate: np.ndarray):\n",
    "                \"\"\"\n",
    "                    TODO: Implement intra-slate dissimilarity metric.\n",
    "                \"\"\"\n",
    "                p = slate.shape[0]\n",
    "\n",
    "                # hamming distance \n",
    "                def hamming_distance(item_1, item_2):\n",
    "                    return np.sum(item_1 != item_2)\n",
    "                \n",
    "                r_div = (p * (p - 1)) / 2 * np.sum([hamming_distance(slate[i], slate[j]) for i in range(p) for j in range(i + 1, p)])\n",
    "\n",
    "\n",
    "                return r_div\n",
    "\n",
    "            r_div = intra_slate_dissimilarity(slate)\n",
    "\n",
    "            def novelty(slate):\n",
    "                \"\"\"\n",
    "                    TODO: Implement novelty metric.\n",
    "                \"\"\"\n",
    "                print(slate)\n",
    "\n",
    "                return 0\n",
    "\n",
    "            r_nov = novelty(slate)\n",
    "\n",
    "            reward = r_acc + r_div + r_nov\n",
    "\n",
    "        return obs, reward, terminated, False, info\n",
    "\n",
    "    def _append_dict_values(self, old_dict, append_dict):\n",
    "        for k in old_dict.keys():\n",
    "            if isinstance(old_dict[k], dict):\n",
    "                self._append_dict_values(old_dict[k], append_dict[k])\n",
    "                continue\n",
    "            old_dict[k].append(append_dict[k])\n",
    "        return\n",
    "\n",
    "    def _to_numpy(self, ep_dict):\n",
    "        for k in ep_dict.keys():\n",
    "            if isinstance(ep_dict[k], dict):\n",
    "                self._to_numpy(ep_dict[k])\n",
    "                continue\n",
    "            ep_dict[k] = np.array(ep_dict[k])\n",
    "        return\n",
    "\n",
    "    def generate_dataset(self, n_users : int, policy: Policy, seed: int = None, dataset_type: _DATASET_FORMATS = \"dict\"):\n",
    "        \"\"\"\n",
    "            Generate a dataset of trajectories from the environment.\n",
    "            If dataset_type in [\"sb3_replay\", sb3_rollout\"], dataset will be a\n",
    "            Replay/Rollout Buffer as in Stable Baselines 3 (requires PyTorch).\n",
    "            Otherwise it is a dictionary of dictionaries.\n",
    "        \"\"\"\n",
    "        dataset = OrderedDict()\n",
    "        print(\"test\")\n",
    "        observation, _ = self.reset(seed = seed)\n",
    "        self.action_space.seed(seed)\n",
    "        u = 0\n",
    "        if dataset_type == \"sb3_rollout\":\n",
    "            try:\n",
    "                import torch\n",
    "            except ModuleNotFoundError:\n",
    "                raise ModuleNotFoundError(\"You need to install pytorch to generate a dataset in DB3 format.\")\n",
    "            from .buffer import DictRolloutBuffer\n",
    "            dataset = DictRolloutBuffer(n_users * self.H,\n",
    "                self.observation_space,\n",
    "                self.action_space,\n",
    "                device = \"cpu\", gamma = 1.0)\n",
    "            ep_starts = np.array(True)\n",
    "        elif dataset_type == \"sb3_replay\":\n",
    "            try:\n",
    "                import torch\n",
    "            except ModuleNotFoundError:\n",
    "                raise ModuleNotFoundError(\"You need to install pytorch to generate a dataset in DB3 format.\")\n",
    "            from .buffer import DictReplayBuffer\n",
    "            dataset = DictReplayBuffer(n_users * self.H,\n",
    "                self.observation_space,\n",
    "                self.action_space,\n",
    "                device = \"cpu\",\n",
    "                handle_timeout_termination = False)\n",
    "        else:\n",
    "            episode_dict = {\"observation\": {\"slate\": [], \"clicks\": [], \"hist\": []},\n",
    "                            \"action\": [],\n",
    "                            \"reward\": []}\n",
    "        while u < n_users:\n",
    "            action = policy.get_action(observation)\n",
    "            print(action)\n",
    "            next_obs, reward, terminated, truncated, info = self.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if dataset_type == \"sb3_rollout\":\n",
    "                dataset.add(observation, info[\"slate\"], reward, ep_starts, torch.zeros(1), torch.ones(1))\n",
    "                ep_starts = np.array(done)\n",
    "            elif dataset_type == \"sb3_replay\":\n",
    "                dataset.add(observation, next_obs, info[\"slate\"], reward, done, None)\n",
    "            else:\n",
    "                self._append_dict_values(episode_dict, {\"observation\": observation, \"action\": info[\"slate\"], \"reward\": reward})\n",
    "\n",
    "            if done:\n",
    "                observation, _ = self.reset()\n",
    "                if dataset_type == \"dict\":\n",
    "                    self._to_numpy(episode_dict)\n",
    "                    dataset[u] = episode_dict\n",
    "                    episode_dict = {\"observation\": {\"slate\": [], \"clicks\": [], \"hist\": []},\n",
    "                            \"action\": [],\n",
    "                            \"reward\": []}\n",
    "                u += 1\n",
    "            else:\n",
    "                observation = next_obs\n",
    "                print(observation)\n",
    "        if dataset_type == \"sb3_rollout\":\n",
    "            dataset.compute_returns_and_advantage(torch.zeros(1), np.array(True))\n",
    "        return dataset\n",
    "    \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from policies import EpsilonGreedyOracle, EpsilonGreedyAntiOracle\n",
    "\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Accessing variables\n",
    "num_items = int(os.getenv('NUM_ITEMS'))\n",
    "num_users = os.getenv('NUM_USERS')\n",
    "num_actions = os.getenv('NUM_ACTIONS')\n",
    "slate_size = int(os.getenv('SLATE_SIZE'))\n",
    "episode_length = int(os.getenv('EPISODE_LENGTH'))\n",
    "env_alpha = float(os.getenv('ENV_ALPHA'))\n",
    "num_topics = int(os.getenv('NUM_TOPICS'))\n",
    "slate_size = int(os.getenv('SLATE_SIZE'))\n",
    "env_alpha = os.getenv('ENV_ALPHA')\n",
    "env_propensities = os.getenv('ENV_PROPENSITIES')\n",
    "env_offset = os.getenv('ENV_OFFSET')\n",
    "env_slope = os.getenv('ENV_SLOPE')\n",
    "env_omega = os.getenv('ENV_OMEGA')\n",
    "recent_items_maxlen = int(os.getenv('RECENT_ITEMS_MAXLEN'))\n",
    "boredom_threshold = float(os.getenv('BOREDOM_THRESHOLD'))\n",
    "boredom_moving_window = int(os.getenv('BOREDOM_MOVING_WINDOW'))\n",
    "env_embedds = os.getenv('ENV_EMBEDDS')\n",
    "click_model = os.getenv('CLICK_MODEL')\n",
    "rel_threshold = os.getenv('REL_THRESHOLD')\n",
    "diversity_penalty = os.getenv('DIVERSITY_PENALTY')\n",
    "diversity_threshold = os.getenv('DIVERSITY_THRESHOLD')\n",
    "print(\"Diversity_threshold\",diversity_threshold)\n",
    "click_prop = float(os.getenv('CLICK_PROP'))\n",
    "boredom_type = os.getenv('BOREDOM_TYPE')\n",
    "rel_penalty = os.getenv('REL_PENALTY')\n",
    "\n",
    "print(f\"num_items: {num_items}, num_users: {num_users}, num_actions: {num_actions}, slate_size: {slate_size}\")\n",
    "\n",
    "env = Sardine(num_items=num_items, \n",
    "                  num_users=num_users, \n",
    "                  num_actions=num_actions, \n",
    "                  slate_size=slate_size, \n",
    "                  episode_length=episode_length,\n",
    "                  seed=2023, \n",
    "                  num_topics=num_topics, \n",
    "                  env_alpha=env_alpha, \n",
    "                  env_propensities=env_propensities,\n",
    "                  env_offset=env_offset, \n",
    "                  env_slope=env_slope,\n",
    "                  env_omega=env_omega,\n",
    "                  recent_items_maxlen=recent_items_maxlen,\n",
    "                  boredom_threshold=boredom_threshold,\n",
    "                  boredom_moving_window=boredom_moving_window,\n",
    "                  env_embedds=env_embedds, \n",
    "                  click_model=click_model, \n",
    "                  rel_threshold=rel_threshold, \n",
    "                  diversity_penalty=diversity_penalty, \n",
    "                  diversity_threshold=int(diversity_threshold), \n",
    "                  click_prop=click_prop, \n",
    "                  boredom_type=boredom_type, \n",
    "                  rel_penalty=rel_penalty,\n",
    "                  )\n",
    "\n",
    "occurrences = env.get_item_occurrences()\n",
    "clicks = env.get_item_clicks()\n",
    "\n",
    "print(occurrences)\n",
    "print(clicks)\n",
    "\n",
    "## Generate a dataset of 10 users with 50% random actions and 50% greedy actions\n",
    "logging_policy = EpsilonGreedyOracle(epsilon = 0.0, env = env, seed = 2023)\n",
    "dataset = env.generate_dataset(n_users = 10, policy = logging_policy, seed = 2023, dataset_type=\"dict\")\n",
    "env.get_item_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sardine",
   "language": "python",
   "name": "sardine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
